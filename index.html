<!DOCTYPE html>
<html>
<body>

  <section id="overview">
    <h2>Overview</h2>
    <p>This project implements LZW (Lempel-Ziv-Welch) compression and decompression using two variations of Trie data structures: a <strong>Standard Trie</strong> and a <strong>Compact Trie</strong>. These data structures serve as dictionaries for LZW, allowing for efficient storage of repeating sequences in the input data. By offering both standard and compact variations, the project enables comparisons in memory usage, compression efficiency, and performance.</p>
    <p>The project provides both compression and decompression functions, along with the ability to run tests using the Gutenberg corpus. Various parameters such as maximum dictionary size, Trie type (Standard or Compact), and bit size can be configured through command-line arguments.</p>
</section>

<section id="components">
    <h2>Components</h2>
    <p>The project is split into the following components:</p>

    <h3>1. Trie Implementations</h3>
    <h4>Standard Trie (trie_standard.py)</h4>
    <ul>
        <li><strong>StandardTrieNode</strong> and <strong>StandardTrie</strong> classes represent the traditional structure of a trie, where each node represents a unique sequence of bytes.</li>
        <li><strong>Functions:</strong>
            <ul>
                <li><strong>insert(sequence, index):</strong> Adds a new sequence to the trie.</li>
                <li><strong>search(sequence):</strong> Searches for a sequence in the trie.</li>
                <li><strong>remove(sequence):</strong> Removes a sequence from the trie.</li>
                <li><strong>visualize_trie():</strong> Recursively displays the entire trie structure.</li>
            </ul>
        </li>
        <li><strong>Memory Management:</strong> The memory usage is tracked using Python's <code>sys.getsizeof()</code> for each node. Each new sequence added to the trie increases the memory footprint proportionally.</li>
    </ul>

    <h4>Compact Trie (trie_compact.py)</h4>
    <ul>
        <li><strong>CompactTrieNode</strong> and <strong>CompactTrie</strong> classes implement a compact version of the trie, reducing memory usage by merging nodes that share common prefixes.</li>
        <li><strong>Functions:</strong>
            <ul>
                <li><strong>insert(sequence, index):</strong> Adds new sequences to the trie while ensuring minimal memory usage by splitting nodes based on common prefixes.</li>
                <li><strong>search(sequence):</strong> Finds sequences, handling partially shared nodes.</li>
                <li><strong>remove(sequence):</strong> Removes a sequence, managing partial matches effectively.</li>
                <li><strong>visualize_trie():</strong> Visualizes the trie structure, providing a compact representation of branches.</li>
            </ul>
        </li>
        <li><strong>Memory Management:</strong> The compact trie uses <code>sys.getsizeof()</code> to keep track of memory use, and shares branches when common prefixes are identified, leading to more efficient memory usage.</li>
    </ul>

    <h3>2. LZW Compression (lzw_compressor.py)</h3>
    <ul>
        <li>The <strong>LZWCompressor</strong> class handles the compression of input data using either the Standard or Compact Trie.</li>
        <li><strong>Parameters: Max Bits, Fixed LZW, Trie Type</strong>
            <ul>
                <li><strong>max_bits:</strong> Defines the maximum number of bits for representing dictionary entries (default is 12 bits).</li>
                <li><strong>use_compact_trie:</strong> Boolean flag to choose between Compact and Standard Trie.</li>
                <li><strong>fixedLZW:</strong> If true, uses a fixed bit length throughout the compression process.</li>
            </ul>
        </li>
        <li><strong>Compression Process:</strong>
            <ul>
                <li>The dictionary is initialized with all possible byte values (0-255).</li>
                <li>Input sequences are added to the trie, and compressed codes are generated based on the existing entries.</li>
                <li><strong>Statistics Generation:</strong> Compression statistics (compression ratio, memory usage, dictionary size, etc.) can be generated optionally.</li>
            </ul>
        </li>
    </ul>

    <h3>3. LZW Decompression (lzw_decompressor.py)</h3>
    <ul>
        <li>The <strong>LZWDecompressor</strong> class decompresses the compressed data back to the original sequence.</li>
        <li><strong>Parameters:</strong> Supports configurable maximum bits and fixed LZW options, similar to the compressor.</li>
        <li><strong>Decompression Process:</strong>
            <ul>
                <li>The dictionary is initialized similarly to the compressor, with individual byte values.</li>
                <li>New sequences are added dynamically as codes are processed.</li>
                <li>The decompression method also allows for generating statistics like execution time, dictionary growth, and memory usage.</li>
            </ul>
        </li>
    </ul>

    <h3>4. Main Script (main.py)</h3>
    <ul>
        <li>The <strong>main.py</strong> script orchestrates the compression and decompression tests, using texts from the Gutenberg corpus.</li>
        <li><strong>Command-Line Arguments:</strong>
            <ul>
                <li><strong>--compact:</strong> Use Compact Trie for compression.</li>
                <li><strong>--fixed:</strong> Use a fixed bit length during compression.</li>
                <li><strong>--stats:</strong> Generate detailed statistics for analysis.</li>
                <li><strong>--gutenberg:</strong> Run tests using texts from the Gutenberg corpus.</li>
                <li><strong>--output:</strong> Save compression and decompression statistics to a CSV file.</li>
            </ul>
        </li>
        <li><strong>Testing Workflow:</strong> The script runs LZW compression and decompression on different lengths of text (short, medium, long, huge) to evaluate the efficiency of both Trie types. The success of compression and decompression is verified by comparing the original text to the decompressed output.</li>
    </ul>
</section>

<section id="usage">
    <h2>Usage</h2>
    <p>The project can be executed via the command line by using <strong>main.py</strong>. Below are some examples:</p>

    <h3>Run Compression and Decompression with Compact Trie</h3>
    <pre><code>python main.py --compact --gutenberg --stats --max-bits 12 --num-files 3 --output stats.csv</code></pre>
    <p>This command will:</p>
    <ul>
        <li>Use the Compact Trie for compression.</li>
        <li>Run tests on the Gutenberg corpus, using the first 3 files.</li>
        <li>Generate statistics and save them to <code>stats.csv</code>.</li>
    </ul>

    <h3>Run with Standard Trie and Fixed Bit Length</h3>
    <pre><code>python main.py --fixed --gutenberg --stats --num-files 2</code></pre>
    <p>This command will:</p>
    <ul>
        <li>Use the Standard Trie.</li>
        <li>Run tests on the first 2 Gutenberg files.</li>
        <li>Use a fixed bit length for LZW compression.</li>
    </ul>
</section>

<section id="performance-comparison">
    <h2>Performance Comparison</h2>
    <ul>
        <li><strong>Standard Trie</strong> offers straightforward insertion and lookup but can be memory-intensive, especially for large datasets.</li>
        <li><strong>Compact Trie</strong> reduces the memory footprint by merging common prefixes, which may result in improved performance for repetitive datasets.</li>
        <li><strong>Compression Ratio & Memory Usage:</strong> The main.py script outputs statistics to help compare memory usage, compression ratio, and execution time between both implementations.</li>
    </ul>
</section>

<section id="results-observations">
    <h2>Results and Observations</h2>
    <ul>
        <li><strong>Memory Efficiency:</strong> The Compact Trie shows significant memory savings due to its prefix-sharing mechanism.</li>
        <li><strong>Speed Trade-Off:</strong> The splitting and merging of nodes in the Compact Trie may lead to slightly longer compression times compared to the Standard Trie for highly diverse datasets.</li>
        <li><strong>Bit Management:</strong> The <code>max_bits</code> parameter influences how the trie grows, and thus impacts the compression efficiency. Fixed bit lengths result in predictable but sometimes less efficient compression.</li>
    </ul>
</section>

<section id="future-improvements">
    <h2>Future Improvements</h2>
    <ul>
        <li><strong>Optimization of Node Splitting:</strong> The Compact Trie could be further optimized to reduce the overhead during node splitting.</li>
        <li><strong>Parallel Compression:</strong> Implementing a multi-threaded approach could reduce execution time, especially for large texts.</li>
        <li><strong>Extended Dataset Evaluation:</strong> Running tests on a more diverse dataset could give a clearer picture of the advantages and trade-offs of both Trie types.</li>
    </ul>
</section>

<section id="contributing">
    <h2>Contributing</h2>
    <p>Feel free to fork this repository and create pull requests to enhance the LZW compression with new features, optimizations, or improved trie structures. Contributions are always welcome!</p>
</section>

<section id="license">
    <h2>License</h2>
    <p>This project is licensed under the MIT License. See the LICENSE file for more information.</p>
</section>

<footer>
    <p><strong>Author:</strong> Raphael</p>
    <p>For further questions or discussions, open an issue on GitHub or reach out via the contact information in the repository.</p>
</footer>
  
</body>
</html>
